# **Parallel 2D Convolution using CUDA**  
### **Optimized Image Processing | C/C++ | CUDA**  

---

## **ğŸ“– Overview**  
This project implements **parallelization of 2D convolution** using **CUDA** for execution on an NVIDIA GPU.
It is an extension of HW3, where convolution was parallelized using **OpenMP**.

In this implementation, three different CUDA-based convolution approaches are explored:  
1. **Naive Parallel Convolution** â€“ Each thread processes a single output pixel using global memory.  
2. **Constant Memory Optimized Convolution** â€“ The kernel is stored in constant memory to reduce memory access latency.  
3. **Tiled Shared Memory Convolution** â€“ Shared memory is used to optimize data locality and reduce redundant memory accesses.  

Performance is analyzed using **Nvprof** to compare execution times for three different implementations.  

ğŸ”¹ **util.cpp**, provided by the TA, handling memory allocation, file reading, and other utilities, while also using the edge-extending code from HW3.

ğŸ”¹ The **"maximum number of threads per block (1024)"** is selected based on the NVIDIA GPU query in Google Colab.

---

## **ğŸ“‚ Directory Layout**  
```bash
ğŸ“¦ CODE
â”œâ”€â”€ ğŸ“‚ data/                  # Input datasets
â”‚   â”œâ”€â”€ image_large.txt         # test/input image
â”‚   â”œâ”€â”€ tiny_kernel.txt         # test kernel
â”‚   â”œâ”€â”€ tiny_kernel2.txt        # test kernel
â”‚   â”œâ”€â”€ tiny_text.txt           # test image
â”œâ”€â”€ ğŸ“‚ src/                   # Source code
â”‚   â”œâ”€â”€ conv_naive.cu           # Basic convolution
â”‚   â”œâ”€â”€ conv_cons.cu            # Optimized convolution (constant memory)
â”‚   â”œâ”€â”€ conv_tiled.cu           # Optimized convolution (shared memory & tiling)
â”‚   â”œâ”€â”€ util.cpp                # Helper functions
â”‚   â”œâ”€â”€ util.h                  # Utility header file
â”œâ”€â”€ ğŸ“‚ bin/                   # Compiled binaries
â”œâ”€â”€ ğŸ“‚ results/               # Output files
â”‚   â”œâ”€â”€ naive_output.txt        # Output from naive convolution
â”‚   â”œâ”€â”€ cons_output.txt         # Output from constant memory convolution
â”‚   â”œâ”€â”€ tiled_output.txt        # Output from tiled convolution
â”œâ”€â”€ ğŸ“œ Makefile               # Build script
â”œâ”€â”€ ğŸ“œ text_to_jpg.py         # Converts output text to an image
```

---

## **âš¡ Compilation & Execution**  
### **1ï¸âƒ£ Prerequisites**  
NVIDIA GPU with CUDA support.\
Ensure that the NVIDIA's CUDA compiler is installed (nvcc is part of the NVIDIA CUDA Toolkit).\
You can check by running:
```bash
nvcc --version
```

Python 3 and the Pillow library for image conversion:
```bash
pip install pillow
```

## **2ï¸âƒ£ Compilation**
Navigate to the project directory and use:
```bash
make
```
This will compile:
- conv_naive â†’ Basic CUDA convolution

- conv_cons â†’ Optimized version using constant memory

- conv_tiled â†’ Optimized version using shared memory & tiling

## **3ï¸âƒ£ Running the Implementations**
â–¶ Running the Naive CUDA Convolution
```bash
make run_naive
```

```bash
make run_cons
```

```bash
make run_tiled
```

Equivalent to:
```bash
./bin/run_{method} data/input.txt data/tiny_kernel2.txt results/naive_output.txt
```
```bash
python3 text_to_jpg.py results/{method}_output.txt
```

**Ex:**  
*tiny_kernel2.txt* (convolution kernel) is applied to *image_large.txt* (input image), and the output is written as *{method}_output.txt*."

Converts the result into an image using text_to_jpg.py


---

To convert an input image to a png file:

```bash
make run_default_image
```

## **4ï¸âƒ£ Profiling with Nvprof**
To analyze performance using Nvprof:

```bash
nvprof ./bin/conv_naive data/input.txt data/tiny_kernel2.txt results/naive_output.txt
nvprof ./bin/conv_cons data/input.txt data/tiny_kernel2.txt results/cons_output.txt
nvprof ./bin/conv_tiled data/input.txt data/tiny_kernel2.txt results/tiled_output.txt
```

## **5ï¸âƒ£ Cleaning Up Build & Results**  
```bash
make clean
```
---  
